<html>
<head>
	<link rel="stylesheet" href="./main.css"> </link>

</head>
<body>
	<div class ="explanation flexbox">

		<h1>Eulerity Take-Home Challenge</h1>

		<h2>How it works</h2>
		<ul>

			<li>Pressing the submit button on this page will make a POST
				request to /main</li>
			<li>That request will contain a form parameter with the url
				populated in the input box below</li>
			<li>The ImageFinder servlet will respond to the request with a
				list of image urls</li>
			<li>This page contains javascript to send the request and use the
				response to build a list of images</li>
			<li>The JavaServlet creates an initial webcrawler called
				<b> crawler </b>that loads the  entire HTML page using a package 
				called 'htmlunit' 
				<ul>
					<li><b>htmlunit</b> uses a class called 
						<b>WebClient</b> to encompass the html page after all 
						of the JavaScript on the respective page is ran. This 
						allows images to be collected from more complex and 
						dynamic pages. 
					</li> 
					<li> WebClient  was created to troubleshoot JS-code. In This
						application, I won't be using that functionality. So I 
						suppressed all related warnings to lower clutter. 
					</li>
					<li><b>To minimize the number of accesses </b> 
						to the site, the WebClient stores the html page 
						using an xml format and writes it to the 
						<i>Resources/output[unique#].txt</i> file. 
					</li>
					<li><b>pom.xml</b> has been configured to clear this directory 
						whenever <b> mvn clean </b> is ran. 
					</li>
					<li> 
						The xml is parsed. The method <b>writeJSON</b> is used 
						to locate any JSON generated via dynamic webpages and stores
						it to: 
						<i>Resources/json_output[unique#].txt</i>
					</li>
					<li>
						Otherwise, the xml is parsed for any common image-related tags via
						the method <b>getElementsHashed()</b>, and also parsed for any 
						files beginning with "https://" (this is done via <b>bruteForceLinkSearch</b>
					</li>
					<li> 
						During the parsing of the xml file, potential subpages are also added 
						to another HashSet, <b>subpages</b>. 
					</li>
				</ul>
			
			</li>
			
			<li>The JavaServlet uses <b> crawler </b> and its HashSet <b> links </b> to create
				a concurrent threadpool of smaller webcrawlers. These smaller webcrawlers
				do not add further subpages. This behavior is specified in the <b>run</b>
				method, since WebCrawler implements <b>Runnable</b> <br/>

				<ul>
					<li> The resulting threads also do not directly add the set of subpages </li>
					<li> They crawl through the subpages and add URLs to a thread-specific hashet</li>
					<li> At the end of <b>run</b> this thread adds a number of random elements to
						the global HashSet </li>
					<li> After all the threads are run, the thread pool is shut down</li> 
				</ul>

			</li>
		<h2> Shortcomings </h2>	
		<ul> 
			<li> 
				However, there are still a few blind spots that I need to address: 
				<ul>
					<li> How can I prevent two identical images, or extremely similar (perhaps
						one may be blurred) from appearing?
					</li>
					<li> 
						While my WebCrawler does manage to crawl some dynamic web pages, 
						it cannot crawl others. This is because people write their JS
						scripts differently, sometimes formatting the JSON differently. 
					</li>
					<li> 
						How do I reduce the number of false positives (URLs that aren't images) 
						without increasing the number of false negatives (URLS without image
						format types that still display images)
					</li>
				</ul>
			</li>
		</ul>
		</ul>
    </div>


	<div class="content">
		<div class="input">
			<form>

				<ul class="form-list">
					<li class ="form-row">
						<label for="url">URL</label>
						<input type="text" name="url" id= "url" placeholder="site url">
					</li>
					<li class ="form-row">
						<label for="threads ">Threads</label>
						<input type="number" name="threads" id="threads" placeholder=3>
					</li>
					<li class ="form-row">
						<label for="per_main">Max Images for Main Page</label>
						<input type="number" name="per_main" id="per_main" placeholder=3>
					</li>
					<li class ="form-row">
						<label for="per_thread">Images per Thread</label>
						<input type="number" name="per_thread" id="per_thread" placeholder=3>
					</li>
					
				</ul> 
				



			</form>
			<button id = "main-page-submit" type="button">Get Images from URL!</button>
		</div>
		
	</div>

	<div class="main-output">
			<ul class="main-results">

			</ul>
	</div>

	<div id="subpage-output">
			<div id="subpage-button-container"> 
				<button id = "subpage-submit" type="button">Get Images from URL's subpages!</button>
			</div>
			
			<ul class="subpage-results">
				
			</ul>
	</div>


	<script>
		var resultList = document.querySelector('ul.main-results');
		var subpageList= document.querySelector('ul.subpage-results');
		var subpages = false;

		var urlInput   = document.querySelector('input[name=url]');
		var numThreads = document.querySelector('input[name=threads]');
		var perMain    = document.querySelector('input[name=per_main]');
		var perThread  = document.querySelector('input[name=per_thread]');

		apiCallBack = function(xhr, callback) {
			if (xhr.readyState == XMLHttpRequest.DONE) {
				// if HTTP request is not done 
				if (xhr.status != 200) {
					let message = xhr.status + ":" + xhr.statusText + ":"
							+ xhr.responseText;
					alert(message);
					// display error message
					throw 'API call returned bad code: ' + xhr.status;
				}

				// if there is responseText, parse it and then call callback on it
				let response = xhr.responseText ? JSON.parse(xhr.responseText)
						: null;
				if (callback) {
					callback(response);
				}
			}
		}

		updateList = function(response) {
			resultList.innerHTML = '';
			for (var i = 0; i < response.length; i++) {
				var img = document.createElement("img");
				img.width = 200;
				img.src = response[i];
				resultList.appendChild(img);
			}
		}

		/**
		@param	url	-- makes call to routing defined in servlet
		@param	method -- GET or POST 
		@param  obj

		*/
		makeApiCall = function(url, method, obj, callback) {
			// instantiates HTTP request
			let xhr = new XMLHttpRequest();

			// initiates a request, adding method and URL
			xhr.open(method, url);

			/* 
			when ready state changes 
			bind the xmlhttprequest and function (i.e update list)
			to the function apiCallBack
			*/
			xhr.onreadystatechange = apiCallBack.bind(null, xhr, callback);

			// will only send form data 
			xhr.send
					(obj ? 

					// if object is form data, object will be returned 
					
					obj instanceof FormData || obj.constructor == String 
						? 
						obj : JSON.stringify(obj) 
					: null);
		}

		// listens 
		document.getElementById('main-page-submit').addEventListener("click", function(event) {
			// prevents it from submitting immediately 
			event.preventDefault();
			subpages = true;
			console.log(subpages); 
			console.log(numThreads.value);

			var _urlInput   = urlInput.value;
			var _numThreads = numThreads.value;
			var _perMain    = perMain.value;
			var _perThread  = perThread.value;

			if(	! _urlInput || 	! _numThreads ||
				! _perMain  ||  ! _perThread  ) {
					window.alert("Please fill out each form. Default values are recommended"); 
					return;
				}

			makeApiCall('/main?url=' + urlInput.value
						+ '&threads=' + numThreads.value 
						+ '&permain=' + perMain.value 
						+ '&perthread=' + perThread.value 
						, 'POST', null , updateList);

			
		});

		document.getElementById('subpage-submit').addEventListener("click", function(event) {
			// prevents it from submitting immediately 
			event.preventDefault();
			console.log("Submit secondary page button");
			console.log(subpages);

			if (! subpages) {
				window.alert("You need to crawl the primary page first");
			} 

			makeApiCall('/main?render_subpage=' + subpages,'POST', null , updateList);
			console.log("See if it is true" + subpages);

			
		});

	</script>
</body>

</html>