<html>
<head>
	<link rel="stylesheet" href="./main.css"> </link>
</head>
<body>
	<div class ="explanation flexbox">

		<h1>Eulerity Take-Home Challenge</h1>

		<h2>How it works</h2>
		<ul>

			<li>Pressing the submit button on this page will make a POST
				request to /main</li>
			<li>That request will contain a form parameter with the url
				populated in the input box below</li>
			<li>The ImageFinder servlet will respond to the request with a
				list of image urls</li>
			<li>This page contains javascript to send the request and use the
				response to build a list of images</li>
			<li>The JavaServlet creates an initial webcrawler called
				<b> crawler </b>that loads the  entire HTML page using a package 
				called 'htmlunit' 
				<ul>
					<li><b>htmlunit</b> uses a class called 
						<b>WebClient</b> to encompass the html page after all 
						of the JavaScript on the respective page is ran. This 
						allows images to be collected from more complex and 
						dynamic pages. 
					</li> 
					<li> WebClient  was created to troubleshoot JS-code. In This
						application, I won't be using that functionality. So I 
						suppressed all related warnings to lower clutter. 
					</li>
					<li><b>To minimize the number of accesses </b> 
						to the site, the WebClient stores the html page 
						using an xml format and writes it to the 
						<i>Resources/output[unique#].txt</i> file. 
					</li>
					<li><b>pom.xml</b> has been configured to clear this directory 
						whenever <b> mvn clean </b> is ran. 
					</li>
					<li> 
						The xml is parsed. The method <b>writeJSON</b> is used 
						to locate any JSON generated via dynamic webpages and stores
						it to: 
						<i>Resources/json_output[unique#].txt</i>
					</li>
					<li>
						Otherwise, the xml is parsed for any common image-related tags via
						the method <b>getElementsHashed()</b>, and also parsed for any 
						files beginning with "https://" (this is done via <b>bruteForceLinkSearch</b>
					</li>
					<li> 
						During the parsing of the xml file, potential subpages are also added 
						to another HashSet, <b>subpages</b>. 
					</li>
				</ul>
			
			</li>
			
			<li>The JavaServlet uses <b> crawler </b> and its HashSet <b> links </b> to create
				a concurrent threadpool of smaller webcrawlers. These smaller webcrawlers
				do not add further subpages. This behavior is specified in the <b>run</b>
				method, since WebCrawler implements <b>Runnable</b> <br/>

				<ul>
					<li> The resulting threads also do not directly add the set of subpages </li>
					<li> They crawl through the subpages and add URLs to a thread-specific hashet</li>
					<li> At the end of <b>run</b> this thread adds a number of random elements to
						the global HashSet </li>
					<li> After all the threads are run, the thread pool is shut down</li> 
				</ul>

			</li>
		<h2> Shortcomings </h2>	
		<ul> 
			<li> 
				However, there are still a few blind spots that I need to address: 
				<ul>
					<li> How can I prevent two identical images, or extremely similar (perhaps
						one may be blurred) from appearing?
					</li>
					<li> 
						While my WebCrawler does manage to crawl some dynamic web pages, 
						it cannot crawl others. This is because people write their JS
						scripts differently, sometimes formatting the JSON differently. 
					</li>
					<li> 
						How do I reduce the number of false positives (URLs that aren't images) 
						without increasing the number of false negatives (URLS without image
						format types that still display images)
					</li>
				</ul>
			</li>
		</ul>
		</ul>
    </div>


	<div class="content">
		<div class="flexbox input">
			<form>
				<input type="text" name="url">
				<input type="text" name="# of subpages">
				<input type="text" name="max images per thread">
			</form>
			<button type="button">Submit!</button>
		</div>
		<div class="output">
			<ul class="results">

			</ul>
		</div>
	</div>
	<script>
		var resultList = document.querySelector('ul.results');
		var urlInput = document.querySelector('input[name=url]')

		apiCallBack = function(xhr, callback) {
			if (xhr.readyState == XMLHttpRequest.DONE) {
				// if HTTP request is not done 
				if (xhr.status != 200) {
					let message = xhr.status + ":" + xhr.statusText + ":"
							+ xhr.responseText;
					alert(message);
					// display error message
					throw 'API call returned bad code: ' + xhr.status;
				}

				// if there is responseText, parse it and then call callback on it
				let response = xhr.responseText ? JSON.parse(xhr.responseText)
						: null;
				if (callback) {
					callback(response);
				}
			}
		}

		updateList = function(response) {
			resultList.innerHTML = '';
			for (var i = 0; i < response.length; i++) {
				var img = document.createElement("img");
				img.width = 200;
				img.src = response[i];
				resultList.appendChild(img);
			}
		}

		/**
		@param	url	-- makes call to routing defined in servlet
		@param	method -- GET or POST 
		@param  obj

		*/
		makeApiCall = function(url, method, obj, callback) {
			// instantiates HTTP request
			let xhr = new XMLHttpRequest();

			// initiates a request, adding method and URL
			xhr.open(method, url);

			/* 
			when ready state changes 
			bind the xmlhttprequest and function (i.e update list)
			to the function apiCallBack
			*/
			xhr.onreadystatechange = apiCallBack.bind(null, xhr, callback);

			// will only send form data 
			xhr.send
					(obj ? 

					// if object is form data, object will be returned 
					
					obj instanceof FormData || obj.constructor == String 
						? 
						obj : JSON.stringify(obj) 
					: null);
		}

		// listens 
		document.querySelector('button').addEventListener("click", function(event) {
			// prevents it from submitting immediately 
			event.preventDefault();

			/*
				located above 
			*/
			makeApiCall('/main?url=' + urlInput.value, 'POST', null, updateList);
		});
	</script>
</body>

</html>